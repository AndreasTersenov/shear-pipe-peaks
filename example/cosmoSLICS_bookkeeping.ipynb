{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define root directory where the simulation files are located\n",
    "root_directory = \"/n17data/tersenov/SLICS/Cosmo_DES\"\n",
    "\n",
    "# Directories to exclude\n",
    "exclude_dirs = [\"SLICS_HR\", \"fid_a\", \"fid_f\"]\n",
    "\n",
    "# Open the master text file in write mode\n",
    "master_file_path = \"master_file.txt\"\n",
    "with open(master_file_path, \"w\") as master_file:\n",
    "    # Iterate over the files and subdirectories in the root directory\n",
    "    for root, dirs, files in os.walk(root_directory):\n",
    "        # Exclude the specified directories\n",
    "        dirs[:] = [d for d in dirs if d not in exclude_dirs]\n",
    "        \n",
    "        for file_name in files:\n",
    "            # Write the file path to the master file\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            master_file.write(file_path + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_files(file_paths):\n",
    "    \"\"\"Reads the information in the filename.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_paths : list of str (.txt file)\n",
    "        List containing the paths to the files to be processed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : recarray\n",
    "        Numpy recarray containing the information extracted from the file names\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Make empty recarray to store the data\n",
    "    data = np.recarray(len(file_paths), dtype=[('id', int), ('seed', 'U1'), ('bin', int), ('LOS', int), ('tile', int)])\n",
    "\n",
    "    # Iterate over the file paths and process each file\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        # Extract the file name from the file path\n",
    "        file_name = file_path.split(\"/\")[-1]\n",
    "        \n",
    "        # Split file name into parts\n",
    "        file_parts = file_name.split(\"_\")\n",
    "        \n",
    "        id = int(file_parts[2])\n",
    "        seed = file_parts[3]\n",
    "        unknown_number = int(file_parts[4])\n",
    "        bin = int(file_parts[5][3:])  # Extract the number after \"Bin\"\n",
    "        LOS = int(file_parts[6][3:])  # Extract the number after \"LOS\"\n",
    "        tile = int(file_parts[7][1:-4])  # Extract the number after \"R\"\n",
    "\n",
    "        # Assign the extracted data to the corresponding fields in the recarray\n",
    "        data[i]['id'] = id\n",
    "        data[i]['seed'] = seed\n",
    "        data[i]['bin'] = bin\n",
    "        data[i]['LOS'] = LOS\n",
    "        data[i]['tile'] = tile\n",
    "        # print(data[i])\n",
    "\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file paths from master_file.txt\n",
    "filename = \"master_file.txt\"\n",
    "with open(filename, 'r') as file:\n",
    "    file_paths = file.readlines()\n",
    "    file_paths = [path.strip() for path in file_paths]\n",
    "\n",
    "data = process_files(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cosmo_params(file_path):\n",
    "    \"\"\"Reads the cosmological parameters from the .dat file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to the .dat file containing the cosmological parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cosmo_params : dict\n",
    "        Dictionary mapping each ID to its corresponding cosmological parameters\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    cosmo_params = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        # Iterate over the lines starting from the second line\n",
    "        for line in lines[1:]:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                parts = line.split()\n",
    "                id = int(parts[0])\n",
    "                params = {\n",
    "                    'Om': float(parts[1]),\n",
    "                    'h': float(parts[2]),\n",
    "                    'w_0': float(parts[3]),\n",
    "                    'sigma_8': float(parts[4]),\n",
    "                    'Oc': float(parts[5])\n",
    "                }\n",
    "                cosmo_params[id] = params\n",
    "\n",
    "    return cosmo_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the .dat file\n",
    "dat_file_path = \"/home/tersenov/shear-pipe-peaks/example/CosmoTable.dat\"\n",
    "\n",
    "# Read the cosmological parameters from the .dat file\n",
    "cosmo_params = read_cosmo_params(dat_file_path)\n",
    "\n",
    "# Map the IDs in the recarray to the corresponding cosmological parameters\n",
    "mapped_params = []\n",
    "for row in data:\n",
    "    id = row['id']\n",
    "    params = cosmo_params.get(id)\n",
    "    if params:\n",
    "        mapped_params.append(params)\n",
    "    else:\n",
    "        print(f\"No parameters found for ID {id}\")\n",
    "\n",
    "# Now, 'mapped_params' will contain the corresponding cosmological parameters for each ID in the recarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the mass map from the catalog, make the S/N maps and count the peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catalog columns:\n",
    "\n",
    "0. RA\n",
    "1. DEC\n",
    "2. e1_data\n",
    "3. e2_data\n",
    "4. w \n",
    "5. redshift_true_sim\n",
    "6. gamma1_sim\n",
    "7. gamma2_sim\n",
    "8. kappa_sim\n",
    "9. s_metacal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "from lenspack.geometry.projections.gnom import radec2xy\n",
    "from lenspack.utils import bin2d\n",
    "from lenspack.image.inversion import ks93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the catalog file\n",
    "catalog_file = \"/n17data/tersenov/SLICS/Cosmo_DES/16_a/LOS4/DES_MocksCat_16_a_4_Bin3_LOS4_R4.dat\"\n",
    "\n",
    "# Load the catalog data as a numpy array\n",
    "catalog_data = np.loadtxt(catalog_file)\n",
    "catalog_data = catalog_data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_catalog_data(catalog_data):\n",
    "    ra = catalog_data[0]\n",
    "    dec = catalog_data[1]\n",
    "    g1_sim = catalog_data[6]\n",
    "    g2_sim = catalog_data[7]\n",
    "    kappa_sim = catalog_data[8]\n",
    "    return ra, dec, g1_sim, g2_sim, kappa_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kappa_map(ra, dec, g1_sim, g2_sim, size_x_deg=10, size_y_deg=10, pixel_size_emap_amin=0.4):\n",
    "    x, y = radec2xy(np.mean(ra), np.mean(dec), ra, dec) # Project (ra,dec) -> (x,y)\n",
    "\n",
    "    Nx = int(size_x_deg / pixel_size_emap_amin * 60)\n",
    "    Ny = int(size_y_deg / pixel_size_emap_amin * 60)\n",
    "\n",
    "    e1map, e2map = bin2d(x, y, npix=(Nx, Ny), v=(g1_sim, g2_sim)) # bin the shear field into a 2D map\n",
    "    emap = np.array([e1map,e2map]) # stack the two components into a single array\n",
    "\n",
    "    kappaE, kappaB = ks93(e1map, -e2map) # make kappa map (the minus sign has to be here for our data conventions)\n",
    "    return kappaE, kappaB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise_to_kappa_map(kappa_map, shape_noise, n_gal, pix_arcmin):\n",
    "    sigma_noise_CFIS = shape_noise / (np.sqrt(2 * n_gal * pix_arcmin**2))\n",
    "    noise_map_CFIS_z05 = sigma_noise_CFIS * np.random.randn(kappa_map.shape[0], kappa_map.shape[1]) # generate noise map\n",
    "    kappa_map_noisy = kappa_map + noise_map_CFIS_z05 # Add noise to the mass map\n",
    "    return kappa_map_noisy, noise_map_CFIS_z05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_kappa_map(kappa_map, pixel_size_emap_amin):\n",
    "    # Set the standard deviation of the Gaussian filter based on the pixel size of the kappa map\n",
    "    precision_Peaks = 2 / pixel_size_emap_amin # pixel_size_emap_amin is the pixel size of the kappa map in arcminutes\n",
    "    kappa_map_smoothed = ndi.gaussian_filter(kappa_map, precision_Peaks)\n",
    "    return kappa_map_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_snr_map(kappa_map_smoothed, noise_map_smoothed):\n",
    "    snr_map = kappa_map_smoothed / np.std(noise_map_smoothed)\n",
    "    return snr_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make SNR map from gaussian-smoothed noisy kappa-map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read catalog data\n",
    "ra, dec, g1_sim, g2_sim, kappa_sim = read_catalog_data(catalog_data)\n",
    "\n",
    "# Create kappa map\n",
    "kappaE, kappaB = create_kappa_map(ra, dec, g1_sim, g2_sim)\n",
    "\n",
    "# Add noise to the kappa map\n",
    "n_gal = 7  # galaxy number density in gal/arcmin^2\n",
    "pix_arcmin = 0.4  # pixel size in arcmin\n",
    "shape_noise = 0.44  # intrinsic ellipticity dispersion\n",
    "kappaE_noisy, noise_map_CFIS_z05 = add_noise_to_kappa_map(kappaE, shape_noise, n_gal, pix_arcmin)\n",
    "\n",
    "# Smooth the noisy kappa map\n",
    "kappaE_noisy_smoothed = smooth_kappa_map(kappaE_noisy, pix_arcmin)\n",
    "\n",
    "# Compute SNR map\n",
    "snr = convert_to_snr_map(kappaE_noisy_smoothed, kappaE_noisy_smoothed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot:\n",
    "1. Noiseless mass map\n",
    "2. Noisy mass map\n",
    "3. Noisy smoothed mass map\n",
    "4. SNR map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot all four maps in subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 7))\n",
    "\n",
    "# Plot the noiseless mass map\n",
    "cmap0 = axs[0, 0].imshow(kappaE, cmap='inferno', vmin=-0.01, vmax=0.01, origin='lower')\n",
    "axs[0, 0].set_title(\"Noiseless Mass Map\")\n",
    "cbar0 = plt.colorbar(cmap0, ax=axs[0, 0])\n",
    "\n",
    "# Plot the noisy mass map\n",
    "cmap1 = axs[0, 1].imshow(kappaE_noisy, cmap='inferno', vmin=-0.1, vmax=0.1, origin='lower')\n",
    "axs[0, 1].set_title(\"Noisy Mass Map\")\n",
    "cbar1 = plt.colorbar(cmap1, ax=axs[0, 1])\n",
    "\n",
    "# Plot the noisy smoothed mass map\n",
    "cmap2 = axs[1, 0].imshow(kappaE_noisy_smoothed, cmap='inferno', vmin=-0.1, vmax=0.1, origin='lower')\n",
    "axs[1, 0].set_title(\"Noisy Smoothed Mass Map\")\n",
    "cbar2 = plt.colorbar(cmap2, ax=axs[1, 0])\n",
    "\n",
    "# Plot the SNR map\n",
    "cmap3 = axs[1, 1].imshow(snr, cmap='inferno', origin='lower')\n",
    "axs[1, 1].set_title(\"SNR Map\")\n",
    "cbar3 = plt.colorbar(cmap3, ax=axs[1, 1])\n",
    "\n",
    "# Reversing x-axis for all plots\n",
    "for ax in axs.flat:\n",
    "    ax.set_xlim(ax.get_xlim()[::-1])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the peak-count histogram from the SNR map and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lenspack.peaks as peaks\n",
    "\n",
    "kappa_snr = np.linspace(-2, 6, 31)  # SNR values to compute peak counts for\n",
    "kappa_th_center_snr = 0.5 * (kappa_snr[:-1] + kappa_snr[1:])  # Center of SNR bins\n",
    "\n",
    "# Compute peak counts for the SNR map\n",
    "kappa_snr = np.linspace(-2, 6, 31)  # Adjust the range as needed\n",
    "peak_counts_first = peaks.peaks_histogram(snr, kappa_snr)[0]\n",
    "\n",
    "# Plot peak counts for the first SNR map\n",
    "plt.plot(kappa_th_center_snr, peak_counts_first)\n",
    "# plt.yscale('log')\n",
    "plt.xlabel('SNR smooth')\n",
    "plt.ylabel('Peak Counts')\n",
    "plt.title('Peak Counts Histogram for Gaussian SNR Map')\n",
    "# plt.ylim(1e-4, 1e4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make multiscale SNR maps with starlet transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lenspack.starlet_l1norm import noise_coeff, get_l1norm_noisy\n",
    "from lenspack.image.transforms import starlet2d\n",
    "from astropy.stats import mad_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_multiscale_snr_maps(image, noise, nscales):\n",
    "    \"\"\"\n",
    "    Compute SNR maps for each wavelet scale of a noisy image.\n",
    "    \n",
    "    Parameters:\n",
    "        image (numpy.ndarray): The noiseless image.\n",
    "        noise (numpy.ndarray): The noise to be added to the image.\n",
    "        nscales (int): Number of wavelet scales for starlet decomposition.\n",
    "        \n",
    "    Returns:\n",
    "        snr_maps (list of numpy.ndarray): List of SNR maps for each scale.\n",
    "    \"\"\"\n",
    "    # Add noise to the noiseless image\n",
    "    image_noisy = image + noise\n",
    "    \n",
    "    # Perform starlet decomposition\n",
    "    image_starlet = starlet2d(image_noisy, nscales)\n",
    "    \n",
    "    # Estimate the noise level\n",
    "    noise_estimate = mad_std(image_noisy)\n",
    "    coeff_j = noise_coeff(image, nscales)\n",
    "    \n",
    "    snr_maps = []\n",
    "    for image_j, std_co in zip(image_starlet, coeff_j):\n",
    "        sigma_j = std_co * noise_estimate\n",
    "        \n",
    "        # Compute SNR map\n",
    "        snr_map = image_j / sigma_j\n",
    "        snr_maps.append(snr_map)\n",
    "    \n",
    "    return snr_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiscale_snr_maps = compute_multiscale_snr_maps(kappaE, noise_map_CFIS_z05, nscales=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the SNR maps of the 5 different scales and the coarse map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SNR maps in subplots with 2 rows\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Define vmin and vmax values for each SNR map\n",
    "vmin_values = [-0.5, -3, -4, -4, -4, -4]  # Replace with your desired vmin values\n",
    "vmax_values = [0.5, 3, 4, 4, 4, 4]  # Replace with your desired vmax values\n",
    "\n",
    "scales_arcmin = [2**(i+1) * pix_arcmin for i in range(len(multiscale_snr_maps))]\n",
    "\n",
    "for i, (snr_map, scale_arcmin) in enumerate(zip(multiscale_snr_maps, scales_arcmin)):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    # cmap = axs[row, col].imshow(snr_map, cmap='inferno', origin='lower')\n",
    "    cmap = axs[row, col].imshow(snr_map, cmap='inferno', origin='lower', vmin = vmin_values[i], vmax = vmax_values[i])\n",
    "    axs[row, col].set_title(f\"SNR Map (Scale {scale_arcmin:.1f} arcmin)\")\n",
    "    cbar = plt.colorbar(cmap, ax=axs[row, col])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the peak counts for each scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_peak_counts_on_snr_maps(snr_maps, kappa_snr):\n",
    "    \"\"\"\n",
    "    Compute peak counts for each wavelet scale of SNR maps.\n",
    "\n",
    "    Parameters:\n",
    "        snr_maps (list of numpy.ndarray): List of SNR maps for each scale.\n",
    "        kappa_snr (numpy.ndarray): Array of kappa values corresponding to SNR maps.\n",
    "\n",
    "    Returns:\n",
    "        kappa_th_center_snr (numpy.ndarray): Array of kappa threshold centers for peak counts.\n",
    "        peak_counts (list of numpy.ndarray): List of peak counts for each scale.\n",
    "    \"\"\"\n",
    "    kappa_th_center_snr = 0.5 * (kappa_snr[:-1] + kappa_snr[1:])\n",
    "    \n",
    "    peak_counts = [peaks.peaks_histogram(snr_map, kappa_snr)[0] for snr_map in snr_maps]\n",
    "\n",
    "    return kappa_th_center_snr, peak_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the peak counts for the different scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_snr = np.linspace(-2, 6, 31)  # SNR values to compute peak counts for\n",
    "\n",
    "# Compute peak counts for each scale\n",
    "kappa_th_center_snr, peak_counts = compute_peak_counts_on_snr_maps(multiscale_snr_maps, kappa_snr)\n",
    "\n",
    "# Plot peak counts for each scale\n",
    "plt.figure()\n",
    "\n",
    "for scale, peak_count in enumerate(peak_counts):\n",
    "    plt.plot(kappa_th_center_snr, peak_count, label=f'Scale {scale+1}')\n",
    "\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "plt.xlabel('SNR smooth')\n",
    "plt.ylabel('Peak Counts')\n",
    "plt.grid(True)\n",
    "plt.xlim(-1, 6)\n",
    "plt.ylim(1e-1, 1e5)\n",
    "plt.title('Peak Counts for Different Scales')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the l1-norm and plot its histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to compute the L1-norm histogram\n",
    "bins_l1, l1norm_histogram = get_l1norm_noisy(kappaE, noise_map_CFIS_z05, nscales=5, nbins=50)\n",
    "\n",
    "# Plot L1-norm histograms for each scale\n",
    "for scale, l1norm_hist in enumerate(l1norm_histogram):\n",
    "    plt.plot(bins_l1[scale], l1norm_hist, label=f'scale {scale}')\n",
    "\n",
    "plt.legend()\n",
    "# plt.yscale('log')\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "plt.xlabel('L1-norm')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "# plt.ylim(1e-4, 1e4)\n",
    "plt.xlim(0, 6)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
